# 自然语言处理期末作业

## 一、作业内容

本次作业内容为基于参考代码 (`seq2seq-rnn.py`)，实现 seq2seq 机器翻译模型。以组队形式完成，每组最多 2 人。参考代码实现了基于普通 RNN 的 seq2seq 模型，其中包括数据加载、训练以及测试流程的实现，评测指标为 BLEU Score。参考代码和相关数据可以从教学网上下载。

### 主要内容

- 在普通 RNN 参考代码的基础上，二选一实现基于 **LSTM** 或 **GRU** 的 seq2seq 模型。
- 在参考代码基础上，实现 seq2seq 模型 decoder 端对 encoder 端的 **attention**。
- **要求不能直接调用 `nn.LSTMCell` 或 `nn.LSTM`**，要基于 `nn.Linear()` 或 `torch.mm()` 实现（可以使用 `torch.softmax`, `torch.tanh` 等激活函数）。
- 请思考实现的 LSTM 或 GRU 代码中，哪些部分实现了什么机制，可以做到比普通 RNN 更长距离的依赖。

### 扩展内容（可选内容，非必须）

- 用 `nn.Linear()` 或 `torch.mm()` 实现基于 **Transformer** 的 seq2seq 机器翻译模型。

### 参考及提示

- **原理介绍：**
  - LSTM 原理介绍：https://colah.github.io/posts/2015-08-Understanding-LSTMs/
  - seq2seq + attention 原理介绍：https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html
  - Transformer 原理介绍：https://theaisummer.com/transformer/?ref=blog.paperspace.com，实践 4-Transformer 模型介绍。
- **代码量预估：**
  - 实现 LSTM（主要内容）大致代码量为 30 行。
  - 实现 attention 机制（主要内容）大致代码量为 20 行。
  - 实现 Transformer seq2seq 模型（可选内容，非必须）大致代码量为 300 行。
- Transformer 模型的推荐设置（例如层数、注意力头数）详见参考代码 README。
- **期望效果：** Transformer > LSTM & GRU > 普通 RNN；有 attention 机制 > 无 attention 机制。参考实验结果详见参考代码的 README。

------

## 二、作业形式

作业形式为**研究报告**，需要包括以下内容：

- 任务描述
- 模型原理介绍和代码实现思路
- 思考题分析
- 实验结果与分析
- 分工
- 参考文献

建议统一用中文书写，对于特殊术语，可考虑在中文后用括号表明对应英文。

------

## 三、作业提交

- **截止日期：** 请看教学网通知
- **提交内容：** 作业需要压缩为压缩包提交，作业文件名命名方式为：`学号-姓名-学号-姓名`（如 `1200011111-张三-1200011112-李四`）。必须包含研究报告、源码及测试结果。
- **提交方式：** 作业的提交方式为上传至教学网（教学网 -> 自然语言处理 -> 教学内容 -> 作业 2）。

------

## 四、作业评分

- 主要对**模型实现合理性、思考题分析合理性、报告详实程度和规范程度**进行评分。
- 其它评分点还包括**作业工作量**与**代码风格**等。
- 鼓励在截止日期之前留一定余量提交。
- 作业可多次提交，成绩以最后一次提交的为准。

------

